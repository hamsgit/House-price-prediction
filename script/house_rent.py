# -*- coding: utf-8 -*-
"""house_rent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AyPKWLMRUt6He4l6FL_3bw3JceqPbPAS
"""

# Install required packages
!pip install pandas scikit-learn

"""Load Dataset into Colab"""

import pandas as pd

# Load dataset
df = pd.read_csv("/content/House_Rent_Dataset.csv")

# Display first 5 rows
print(df.head())

# Show dataset shape (rows, columns)
print("Shape of dataset:", df.shape)

"""Explore Data (EDA Basics)"""

# See column names
print("Columns:", df.columns)

# Data types & missing values
print("\nDataset Info:")
print(df.info())

# Summary statistics for numeric columns
print("\nSummary Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

"""#Preprocessing Plan



Drop irrelevant columns:

Posted On (date not useful now)

Point of Contact (not useful for prediction)

Handle “Floor” column:

Example: "Ground out of 2", "3 out of 5".

We’ll split into two numeric features:

Current Floor (e.g., Ground = 0, 3 = 3)

Total Floors (e.g., 2, 5, etc.)

Encode categorical features:

Area Type, City, Furnishing Status, Tenant Preferred → convert to numbers using Label Encoding / OneHotEncoding.

Area Locality has too many unique values → we’ll drop it (too detailed, won’t help model).
"""

# Drop irrelevant columns
df = df.drop(["Posted On", "Point of Contact", "Area Locality"], axis=1)

# Handle Floor column
df[['Current Floor', 'Total Floors']] = df['Floor'].str.split(' out of ', expand=True)

# Replace "Ground" with 0 and convert to numeric
df['Current Floor'] = df['Current Floor'].replace('Ground', 0)
df['Current Floor'] = pd.to_numeric(df['Current Floor'], errors='coerce')
df['Total Floors'] = pd.to_numeric(df['Total Floors'], errors='coerce')

# Drop old Floor column
df = df.drop('Floor', axis=1)

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder

categorical_cols = ['Area Type', 'City', 'Furnishing Status', 'Tenant Preferred']

le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Check processed dataset
print(df.head())
print("\nProcessed Shape:", df.shape)

"""Train-Test Split"""

from sklearn.model_selection import train_test_split

# Features (X) and Target (y)
X = df.drop("Rent", axis=1)
y = df["Rent"]

# Train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

"""Scaling the data"""

from sklearn.preprocessing import StandardScaler

# Initialize scaler
scaler = StandardScaler()

# Fit on training data, transform both train and test
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Scaled Training Data Shape:", X_train_scaled.shape)
print("Scaled Test Data Shape:", X_test_scaled.shape)

"""Linear Regression

Check for NaNs
"""

import numpy as np

print("NaNs in training set:", np.isnan(X_train_scaled).sum())
print("NaNs in test set:", np.isnan(X_test_scaled).sum())

"""Fill NaNs with median of the column"""

from sklearn.impute import SimpleImputer

# Initialize imputer to fill NaNs with median
imputer = SimpleImputer(strategy='median')

# Fit on training data and transform both train and test
X_train_scaled = imputer.fit_transform(X_train_scaled)
X_test_scaled = imputer.transform(X_test_scaled)

# Check again for NaNs
import numpy as np
print("NaNs in training set after imputation:", np.isnan(X_train_scaled).sum())
print("NaNs in test set after imputation:", np.isnan(X_test_scaled).sum())

"""Train Linear Regression Model"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Initialize Linear Regression
lin_reg = LinearRegression()

# Train the model
lin_reg.fit(X_train_scaled, y_train)

# Predictions
y_pred_train = lin_reg.predict(X_train_scaled)
y_pred_test = lin_reg.predict(X_test_scaled)

# Evaluate
mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

print("Linear Regression Results:")
print("Train MSE:", mse_train)
print("Test MSE:", mse_test)
print("Train R²:", r2_train)
print("Test R²:", r2_test)

"""Train Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

# Initialize model
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)

# Train
rf_reg.fit(X_train_scaled, y_train)

# Predictions
y_pred_train_rf = rf_reg.predict(X_train_scaled)
y_pred_test_rf = rf_reg.predict(X_test_scaled)

# Evaluation
mse_train_rf = mean_squared_error(y_train, y_pred_train_rf)
mse_test_rf = mean_squared_error(y_test, y_pred_test_rf)
r2_train_rf = r2_score(y_train, y_pred_train_rf)
r2_test_rf = r2_score(y_test, y_pred_test_rf)

print("Random Forest Results:")
print("Train MSE:", mse_train_rf)
print("Test MSE:", mse_test_rf)
print("Train R²:", r2_train_rf)
print("Test R²:", r2_test_rf)

import joblib

# Save Random Forest model
joblib.dump(rf_reg, "house_rent_rf_model.pkl")

# Save the scaler
joblib.dump(scaler, "scaler.pkl")

print("Model and scaler saved successfully!")

import pandas as pd

# Example new data
new_data = pd.DataFrame({
    'BHK': [2],
    'Size': [950],
    'Area Type': [2],           # Encoded same as training (LabelEncoder)
    'City': [4],                # Encoded same as training
    'Furnishing Status': [1],   # Encoded same as training
    'Tenant Preferred': [1],    # Encoded same as training
    'Bathroom': [2],
    'Current Floor': [1],
    'Total Floors': [3]
})

import joblib

# Load model and scaler
model = joblib.load("house_rent_rf_model.pkl")
scaler = joblib.load("scaler.pkl")

new_data_scaled = scaler.transform(new_data)

predicted_rent = model.predict(new_data_scaled)
print("Predicted Rent:", predicted_rent[0])

import pandas as pd

# Example: 3 new houses
new_data_batch = pd.DataFrame({
    'BHK': [2, 3, 1],
    'Size': [950, 1200, 500],
    'Area Type': [2, 2, 1],          # Encoded same as training
    'City': [4, 4, 3],               # Encoded same as training
    'Furnishing Status': [1, 2, 0],  # Encoded same as training
    'Tenant Preferred': [1, 1, 0],   # Encoded same as training
    'Bathroom': [2, 3, 1],
    'Current Floor': [1, 2, 0],
    'Total Floors': [3, 5, 2]
})

new_data_scaled = scaler.transform(new_data_batch)
predicted_rents = model.predict(new_data_scaled)

# Display results
new_data_batch['Predicted Rent'] = predicted_rents
print(new_data_batch)

from google.colab import files

files.download("house_rent_rf_model.pkl")
files.download("scaler.pkl")